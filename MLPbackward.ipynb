{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-29T22:47:13.982074Z",
     "start_time": "2018-09-29T22:47:13.672095Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y(y_pred, y_true):\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_true = y_true.reshape(-1)\n",
    "    order = np.argsort(y_pred)\n",
    "    plt.plot(y_true[order], 'o ', label=u'réel')\n",
    "    plt.plot(y_pred[order], 'P ', label=u'prédiction')\n",
    "    plt.legend()\n",
    "    plt.xlabel(u'échantillons')\n",
    "    plt.ylabel('valeur de y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization d'un MLP\n",
    "\n",
    "Dans cet exercice nous allons réaliser l'optimization d'un perceptron à une couche caché. \n",
    "On se servira de pytorch et sa fonctionalité autograd pour apliquer la descente du gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données : Predire la progression du diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On continuera à travailler avec les données sur la progression du diabetes. Le code ci dessus charge les données dans des variables, les normalize et les divide en \"train\" et \"test\", exactement comme dans le notebook antérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "\n",
    "#print(data.DESCR)\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# import pandas as pd\n",
    "# pd.DataFrame(X[:10,:], columns=data.feature_names)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "y_train, y_test = y_train.reshape([-1,1]), y_test.reshape([-1,1])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP en pytorch\n",
    "\n",
    "Ici je reprends le code pour le MLP qui à été fait dans le notebook antérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X_train.shape[0]  # nombre d'échantillons\n",
    "D = X_train.shape[1]  # nombre de features\n",
    "S = y_train.shape[1]  # nombre de sorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(D, S, N=5):\n",
    "    rng = np.random.RandomState(0)\n",
    "    \n",
    "    W = torch.from_numpy(rng.rand(D,N))\n",
    "    b = torch.zeros((N,1), dtype=torch.float64)\n",
    "    \n",
    "    O = torch.from_numpy(rng.rand(N,S))\n",
    "\n",
    "    return W, b, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X, W, b, O):\n",
    "    M, D = X.shape  # nombre d'échantillons, nombre de features\n",
    "    N = W.shape[1]  # N le nombre de neurones de la couche caché\n",
    "    \n",
    "    # On transforme X en un torch.Tensor\n",
    "    X = torch.from_numpy(X)\n",
    "    \n",
    "    # np.dot devient torch.mm (matrix multiplication)\n",
    "    H = torch.mm(X,W)+b.transpose(1,0)\n",
    "    A = torch.tanh(H)\n",
    "    Y = torch.mm(A,O)\n",
    "\n",
    "    \n",
    "    # Ici quelques verifications sur la taille des matrices pour vous aider\n",
    "    try:\n",
    "        assert(H.shape == (M,N))\n",
    "    except AssertionError:\n",
    "        print(\"Taille de H semble erronée:\",H.shape, \", ça devrait être\", (M,N))    \n",
    "\n",
    "    try:\n",
    "        assert(Y.shape == (M,S))\n",
    "    except AssertionError:\n",
    "        print(\"Taille de Y semble erronée:\",Y.shape, \", ça devrait être\", (M,S))\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cout(Y_pred, Y_true):\n",
    "    M = Y_true.shape[0]\n",
    "    Y_true = torch.from_numpy(Y_true)\n",
    "    # completez le code avec l'expression pour la fonction de cout J\n",
    "    J = (1/M)*((Y_pred-Y_true)**2).sum()\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, O = parameters(D, S, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = MLP(X_train, W, b, O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = cout(Y_pred, y_train)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retro-propagation avec Autograd\n",
    "\n",
    "Ici on va implementer la retro-propagation en s'apuiant sur autograd. Pour cela il faut habiliter le calcul de gradient sur les tensors qui determinent nos paramètres W, b et O.\n",
    "Pour cela, il faudra maquer leur attribut `requires_grad` comme vrai: `requires_grad=True`.\n",
    "Ceci est fait ci-dessus pour chacun des paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(D, S, N=5):\n",
    "    rng = np.random.RandomState(0)\n",
    "    \n",
    "    # ici on met requires_grad = True sur le tensor directement\n",
    "    W = torch.from_numpy(rng.rand(D,N))\n",
    "    W.requires_grad = True\n",
    "\n",
    "    # ici on peu le passer en tant que mot-cle dans la création de b\n",
    "    b = torch.zeros((N,1), dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # une 3eme façon de le faire ici\n",
    "    O = torch.from_numpy(rng.rand(N,S))\n",
    "    O.requires_grad_(True)\n",
    "\n",
    "    return W, b, O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut les re-créer avec ce nouveau format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, O = parameters(D, S, N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et ensuite recalculer les predicions et la fonction de cout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = MLP(X_train, W, b, O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = cout(Y_pred, y_train)\n",
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La fonction `backward`\n",
    "En apelant la methode backward sur la fonction de cout, les gradients seront calculés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on peut consulter les valeurs obtenues pour les gradients\n",
    "- `W.grad` est $\\nabla_W J$\n",
    "- `O.grad`est  $\\nabla_O J$ \n",
    "- et `b.grad` est $\\nabla_b J$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `backward` en fait calcule les gradients et les somme aux valeurs qui sont déjà dedans les variables `grad` de chaque paramètre. Pour cela, si on veut les recalculer, il faut d'abord remettre les variables `grad` à zéro, comme fait dans la fonction ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grads(W, b, O):\n",
    "    W.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    O.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on appèlle cette fonction, les gradients seront mis à zéro, comme vous pouvez le vérifier ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_grads(W, b, O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad, b.grad, O.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : pas du gradient\n",
    "\n",
    "Maintenant vous savez comment calculer les gradients et voir leurs valeurs. A vous de remplir la fonction cidessus avec les mises à jour pour la descente du gradient.\n",
    "Vous allez metre à jour W, b et O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(learning_rate, W,b,O):\n",
    "    lr = learning_rate\n",
    "    # Metez à jour W, b et O \n",
    "    # dans la direction oposé du gradient\n",
    "    # Ca prendra la forme\n",
    "    # Tensor.data = Tensor.data - lr * Tensor.grad\n",
    "    W.data = W.data - lr * W.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : boucle d'optimization\n",
    "Tout est prêt pour l'entrainement de notre réseau. Il faut maintenant créer la boucle d'optimization qui realise la descente de gradient pour W, b, et O. Suivez les indications et completez le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, o = parameters(D, S, N=5) # N est le nombre de neurones de la couche caché\n",
    "max_iterations = 1000\n",
    "learning_rate = 1e-5\n",
    "cost_curve = []\n",
    "for i in range(max_iterations):\n",
    "    #Completez le code ci dessous (numeros 1 a 5)\n",
    "    # 1) calculez les predictions avec le réseau (forward pass)\n",
    "    Y_pred\n",
    "    \n",
    "    # 2) Calculez la fonction de cout\n",
    "    J = \n",
    "    \n",
    "    # Sauvegarder J pour plot\n",
    "    cost_curve.append(J.detach().numpy())\n",
    "    \n",
    "    # 3) Calculez les gradients avec la fonction backward()\n",
    "    \n",
    "\n",
    "    # 4) metez a jour W, b et o avec la fonction gradient_step\n",
    "    \n",
    "    \n",
    "    # 5) mettez à zero les variables grad avant le prochain pas\n",
    "    #    avec la fonction zero_grads()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tout se passe bien vous allez voir ci-dessus l'evolution de la valeur de la focntion de coût au long des iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_curve)\n",
    "plt.title(\"courbe d'apprentissage\")\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y(Y_pred.detach().numpy(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du modèle\n",
    "\n",
    "Finalement, on poura tester la qualité de notre modéle sur notre ensemble de teste ( qui n'a pas été utilisé pour l'apprentissage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = MLP(X_test, W, b, O)\n",
    "J = cout(Y_pred,y_test)\n",
    "print(\"cout sur l'ensemble de test\", J.detach().numpy())\n",
    "plot_y(Y_pred.detach().numpy(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : essayez d'augmenter le nombre d'iterations, de neurones ou changer la learning rate pour voir si le modèle peut mieux faire !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrigés\n",
    "### Gradient step\n",
    "``` python\n",
    "def gradient_step(learning_rate, W,b,O):\n",
    "    lr = learning_rate\n",
    "    # Metez à jour W, b et O \n",
    "    # dans la direction oposé du gradient\n",
    "    # Ca prendra la forme\n",
    "    # Tensor.data = Tensor.data - lr * Tensor.grad\n",
    "    W.data = W.data - lr * W.grad\n",
    "    b.data = b.data - lr * b.grad\n",
    "    O.data = O.data - lr * O.grad\n",
    "```    \n",
    "### Boucle d'entrainement\n",
    "``` python\n",
    "for i in range(max_iterations):\n",
    "    #Completez le code ci dessous (numeros 1 a 5)\n",
    "    # 1) calculez les predictions avec le réseau (forward pass)\n",
    "    Y_pred = MLP(X_train, W, b, O)\n",
    "    \n",
    "    # 2) Calculez la fonction de cout\n",
    "    J = (Y_pred, y_train)\n",
    "    \n",
    "    # Sauvegarder J pour plot\n",
    "    cost_curve.append(J.detach().numpy())\n",
    "    \n",
    "    # 3) Calculez les gradients avec la fonction backward()\n",
    "    J.backward()\n",
    "\n",
    "    # 4) metez a jour W, b et o avec la fonction gradient_step\n",
    "    gradient_step(W, b, o)\n",
    "    \n",
    "    # 5) mettez à zero les variables grad avant le prochain pas\n",
    "    #    avec la fonction zero_grads()\n",
    "    zero_grads(W, b, o)\n",
    "```     \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "rise": {
   "theme": "simple",
   "transition": "none"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "327.009px",
    "width": "355.999px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.997px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.085px",
    "left": "1310.04px",
    "right": "20px",
    "top": "161.992px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
